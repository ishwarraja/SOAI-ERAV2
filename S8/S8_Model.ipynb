{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNePrX3aBi3jIcqJJixvt8Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ishwarraja/SOAI-ERAV2/blob/main/S8/S8_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "wjAeKgZ0qfw4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "xoAhQdlPrxAR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define transformations to apply to the dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize the images\n",
        "])\n",
        "\n"
      ],
      "metadata": {
        "id": "xpGJnf1Eq1Ez"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the CIFAR10 train dataset\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True, num_workers=2)\n",
        "\n",
        "# Load the CIFAR10 test dataset\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "test_loader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iuURNJ0_q3d0",
        "outputId": "5894ac1f-3bd8-4d65-fe8a-d68d7fd1c27a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:01<00:00, 102822650.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the class labels\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "metadata": {
        "id": "aoZjJhLRq574"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class GroupNormNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GroupNormNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(128, 256, 3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv4 = nn.Conv2d(256, 512, 3, padding=1)\n",
        "        self.conv5 = nn.Conv2d(512, 512, 3, padding=1)\n",
        "        self.conv6 = nn.Conv2d(512, 256, 3, padding=1)\n",
        "        self.conv7 = nn.Conv2d(256, 128, 3, padding=1)\n",
        "        self.conv8 = nn.Conv2d(128, 64, 3, padding=1)\n",
        "        self.conv9 = nn.Conv2d(64, 10, 3, padding=1)\n",
        "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = self.pool(x)\n",
        "        x = self.pool(F.relu(self.conv4(x)))\n",
        "        x = F.relu(self.conv5(x))\n",
        "        x = F.relu(self.conv6(x))\n",
        "        x = self.pool(F.relu(self.conv7(x)))\n",
        "        x = F.relu(self.conv8(x))\n",
        "        x = self.conv9(x)\n",
        "        x = self.gap(x)\n",
        "        x = x.view(-1, 10)\n",
        "        return F.log_softmax(x, dim=-1)\n"
      ],
      "metadata": {
        "id": "vEp6Jp8UrITf"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class LayerNormNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LayerNormNet, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv5 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv6 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        self.conv7 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv8 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv9 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "        self.fc10 = nn.Linear(512, 10)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.layernorm = nn.LayerNorm([64, 32, 32])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.conv1(x))\n",
        "        x = self.relu(self.conv2(x))\n",
        "        x = self.relu(self.conv3(x))\n",
        "        x = self.pool(x)\n",
        "\n",
        "        x = self.relu(self.conv4(x))\n",
        "        x = self.relu(self.conv5(x))\n",
        "        x = self.relu(self.conv6(x))\n",
        "        x = self.pool(x)\n",
        "\n",
        "        x = self.relu(self.conv7(x))\n",
        "        x = self.relu(self.conv8(x))\n",
        "        x = self.relu(self.conv9(x))\n",
        "        x = self.pool(x)\n",
        "\n",
        "        x = self.global_avg_pool(x)\n",
        "        x = x.view(-1, 512)\n",
        "        x = self.fc10(x)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "nhZ4FZuzwfau"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "'''\n",
        "# Define the network architectures\n",
        "\n",
        "# Network with Group Normalization\n",
        "class GroupNormNet(nn.Module):\n",
        "    # Architecture definition here...\n",
        "\n",
        "# Network with Layer Normalization\n",
        "class LayerNormNet(nn.Module):\n",
        "    # Architecture definition here...\n",
        "\n",
        "# Network with Batch Normalization\n",
        "class BatchNormNet(nn.Module):\n",
        "    # Architecture definition here...\n",
        "\n",
        "\n",
        "'''\n",
        "\n",
        "class BatchNormNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BatchNormNet, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(128)\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(512)\n",
        "        self.conv5 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn5 = nn.BatchNorm2d(512)\n",
        "        self.conv6 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn6 = nn.BatchNorm2d(512)\n",
        "\n",
        "        self.conv7 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn7 = nn.BatchNorm2d(512)\n",
        "        self.conv8 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn8 = nn.BatchNorm2d(512)\n",
        "        self.conv9 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn9 = nn.BatchNorm2d(512)\n",
        "\n",
        "        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "        self.fc10 = nn.Linear(512, 10)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.relu(self.bn2(self.conv2(x)))\n",
        "        x = self.relu(self.bn3(self.conv3(x)))\n",
        "        x = self.pool(x)\n",
        "\n",
        "        x = self.relu(self.bn4(self.conv4(x)))\n",
        "        x = self.relu(self.bn5(self.conv5(x)))\n",
        "        x = self.relu(self.bn6(self.conv6(x)))\n",
        "        x = self.pool(x)\n",
        "\n",
        "        x = self.relu(self.bn7(self.conv7(x)))\n",
        "        x = self.relu(self.bn8(self.conv8(x)))\n",
        "        x = self.relu(self.bn9(self.conv9(x)))\n",
        "        x = self.pool(x)\n",
        "\n",
        "        x = self.global_avg_pool(x)\n",
        "        x = x.view(-1, 512)\n",
        "        x = self.fc10(x)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "WFqJ8i1ZxCZ3"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "# Training parameters\n",
        "max_epochs = 20\n",
        "\n",
        "# Training loop for Group Normalization Network\n",
        "model = GroupNormNet().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "for epoch in range(max_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    train_accuracy = 100.0 * correct / total\n",
        "\n",
        "    # Evaluate on test set\n",
        "    model.eval()\n",
        "    test_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    test_accuracy = 100.0 * correct / total\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{max_epochs}]: Train Loss: {train_loss:.3f}, Train Accuracy: {train_accuracy:.2f}%, Test Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "# Training loop for Layer Normalization Network\n",
        "# Same code structure as above, replacing the model definition with LayerNormNet and updating the print statements accordingly.\n",
        "\n",
        "# Training loop for Batch Normalization Network\n",
        "# Same code structure as above, replacing the model definition with BatchNormNet and updating the print statements accordingly.\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSPVWBLvrdRP",
        "outputId": "cb197dae-323b-491d-c075-ad461e190c6a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20]: Train Loss: 3599.210, Train Accuracy: 9.77%, Test Accuracy: 10.00%\n",
            "Epoch [2/20]: Train Loss: 3599.124, Train Accuracy: 9.94%, Test Accuracy: 10.00%\n",
            "Epoch [3/20]: Train Loss: 3599.129, Train Accuracy: 9.64%, Test Accuracy: 10.00%\n",
            "Epoch [4/20]: Train Loss: 3599.124, Train Accuracy: 9.81%, Test Accuracy: 10.00%\n",
            "Epoch [5/20]: Train Loss: 3599.110, Train Accuracy: 9.98%, Test Accuracy: 10.00%\n",
            "Epoch [6/20]: Train Loss: 3599.123, Train Accuracy: 9.78%, Test Accuracy: 10.00%\n",
            "Epoch [7/20]: Train Loss: 3599.100, Train Accuracy: 9.91%, Test Accuracy: 10.00%\n",
            "Epoch [8/20]: Train Loss: 3599.129, Train Accuracy: 9.77%, Test Accuracy: 14.54%\n",
            "Epoch [9/20]: Train Loss: 3599.120, Train Accuracy: 9.89%, Test Accuracy: 10.00%\n",
            "Epoch [10/20]: Train Loss: 3599.106, Train Accuracy: 9.76%, Test Accuracy: 10.00%\n",
            "Epoch [11/20]: Train Loss: 3599.080, Train Accuracy: 9.71%, Test Accuracy: 10.00%\n",
            "Epoch [12/20]: Train Loss: 3599.127, Train Accuracy: 9.74%, Test Accuracy: 10.00%\n",
            "Epoch [13/20]: Train Loss: 3599.088, Train Accuracy: 9.89%, Test Accuracy: 10.00%\n",
            "Epoch [14/20]: Train Loss: 3599.117, Train Accuracy: 9.70%, Test Accuracy: 10.00%\n",
            "Epoch [15/20]: Train Loss: 3599.114, Train Accuracy: 9.80%, Test Accuracy: 10.00%\n",
            "Epoch [16/20]: Train Loss: 3599.113, Train Accuracy: 9.84%, Test Accuracy: 10.00%\n",
            "Epoch [17/20]: Train Loss: 3599.102, Train Accuracy: 9.90%, Test Accuracy: 10.00%\n",
            "Epoch [18/20]: Train Loss: 3599.123, Train Accuracy: 9.78%, Test Accuracy: 10.00%\n",
            "Epoch [19/20]: Train Loss: 3599.056, Train Accuracy: 9.94%, Test Accuracy: 10.00%\n",
            "Epoch [20/20]: Train Loss: 3599.110, Train Accuracy: 9.95%, Test Accuracy: 10.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "# Define the architectures and load the trained models\n",
        "group_norm_model = GroupNormNet().to(device)\n",
        "group_norm_model.load_state_dict(torch.load('/content/test/group_norm_model.txt'))\n",
        "\n",
        "layer_norm_model = LayerNormNet().to(device)\n",
        "layer_norm_model.load_state_dict(torch.load('/content/test/layer_norm_model.txt'))\n",
        "\n",
        "batch_norm_model = BatchNormNet().to(device)\n",
        "batch_norm_model.load_state_dict(torch.load('/content/test/batch_norm_model.txt'))\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate_model(model):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    accuracy = 100.0 * correct / total\n",
        "    return accuracy\n",
        "\n",
        "# Evaluate the models on the test dataset and record the accuracy\n",
        "group_norm_accuracy = evaluate_model(group_norm_model)\n",
        "layer_norm_accuracy = evaluate_model(layer_norm_model)\n",
        "batch_norm_accuracy = evaluate_model(batch_norm_model)\n",
        "\n",
        "print(\"Accuracy on the test dataset:\")\n",
        "print(\"Group Normalization: {:.2f}%\".format(group_norm_accuracy))\n",
        "print(\"Layer Normalization: {:.2f}%\".format(layer_norm_accuracy))\n",
        "print(\"Batch Normalization: {:.2f}%\".format(batch_norm_accuracy))\n",
        "\n",
        "# Find misclassified images for Batch Normalization model\n",
        "batch_norm_model.eval()\n",
        "misclassified_images = []\n",
        "misclassified_labels = []\n",
        "correct_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = batch_norm_model(inputs)\n",
        "        _, predicted = outputs.max(1)\n",
        "        misclassified_mask = predicted.ne(labels)  # Mask for misclassified images\n",
        "        misclassified_images.extend(inputs[misclassified_mask].cpu())\n",
        "        misclassified_labels.extend(predicted[misclassified_mask].cpu())\n",
        "        correct_labels.extend(labels[misclassified_mask].cpu())\n",
        "\n",
        "# Display misclassified images\n",
        "num_samples = 10\n",
        "misclassified_images = misclassified_images[:num_samples]\n",
        "misclassified_labels = misclassified_labels[:num_samples]\n",
        "correct_labels = correct_labels[:num_samples]\n",
        "\n",
        "fig, axs = plt.subplots(5, 2, figsize=(10, 15))\n",
        "fig.suptitle(\"Misclassified Images (Batch Normalization Model)\")\n",
        "\n",
        "for i in range(num_samples):\n",
        "    image = misclassified_images[i].permute(1, 2, 0)\n",
        "    image = (image * 0.5) + 0.5  # Denormalize\n",
        "    label_pred = misclassified_labels[i].item()\n",
        "    label_true = correct_labels[i].item()\n",
        "    axs[i // 2, i % 2].imshow(image)\n",
        "    axs[i // 2, i % 2].set_title(f\"Predicted: {label_pred}, True: {label_true}\")\n",
        "    axs[i // 2, i % 2].axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        },
        "id": "pKyYt4STv9S8",
        "outputId": "f4e0ce96-3d32-405a-ed63-e283c5c6a1a8"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "EOFError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-6b68c6ea1bc9>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Define the architectures and load the trained models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mgroup_norm_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGroupNormNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mgroup_norm_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/test/group_norm_model.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlayer_norm_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLayerNormNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    813\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    814\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mUNSAFE_MESSAGE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 815\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    816\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_legacy_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1031\u001b[0m             \"functionality.\")\n\u001b[1;32m   1032\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1033\u001b[0;31m     \u001b[0mmagic_number\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1034\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmagic_number\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mMAGIC_NUMBER\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid magic number; corrupt file?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mEOFError\u001b[0m: Ran out of input"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mkdir test\n",
        "!cd /content/test\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kO3gFPWexoHh",
        "outputId": "a7ca2c58-27a3-47b6-c140-9e695c4bec03"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/test\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!touch group_norm_model.txt layer_norm_model.txt group_norm_model.txt"
      ],
      "metadata": {
        "id": "3mVCI8Tnxo-A"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Raqd2lTKysVJ",
        "outputId": "332c37d8-691b-4ede-c6ce-960cae0815c1"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 0\n",
            "-rw-r--r-- 1 root root 0 Jun 21 16:30 group_norm_model.txt\n",
            "-rw-r--r-- 1 root root 0 Jun 21 16:30 layer_norm_model.txt\n",
            "-rw-r--r-- 1 root root 0 Jun 21 16:30 batch_norm_model.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NDs_9o6Pz7DE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}